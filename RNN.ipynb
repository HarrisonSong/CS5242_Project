{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "from keras.layers import Embedding, ELU, Dropout, Flatten, Input, Dense, BatchNormalization, SpatialDropout1D, LSTM\n",
    "from keras.layers import concatenate, multiply\n",
    "from keras.layers import TimeDistributed, Reshape, RepeatVector, Lambda, Activation\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "data = pd.read_csv(\"sample_data.csv\",header=None,dtype='float64',names=list(range(4096))).fillna(0)\n",
    "test = pd.read_csv('sample_data.csv',header=None,dtype='float64',names=list(range(4096))).fillna(0)\n",
    "label = pd.read_csv('sample_label.csv')['category'].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180/500 [=========>....................] - ETA: 2:39 - loss: 0.8395 - binary_accuracy: 0.5111"
     ]
    }
   ],
   "source": [
    "maxlen=4096\n",
    "embed_size=16\n",
    "lstm_layer_size = 10\n",
    "num_layers = 3\n",
    "\n",
    "main_input = Input(shape=(maxlen,), name='main_input')\n",
    "emb = Embedding(256, embed_size, input_length=maxlen, embeddings_regularizer=l2(1e-4))(main_input)\n",
    "# drop = SpatialDropout1D(0.2)(emb)\n",
    "hs = [] #hidden states from each LSTM layer stored here \n",
    "lstm = LSTM(lstm_layer_size, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(1e-5), recurrent_regularizer=l2(1e-5), return_sequences=True)(emb)\n",
    "hs.append(lstm)\n",
    "for l in range(1, num_layers):\n",
    "    hs.append(LSTM(lstm_layer_size, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(1e-5), recurrent_regularizer=l2(1e-5), return_sequences=True)(hs[-1]))\n",
    "local_states = concatenate(hs)\n",
    "average_active = Lambda(function=lambda x: K.mean(x, axis=1), output_shape=lambda shape: (shape[0],) + shape[2:])(local_states) # this produces h\n",
    "state_size = lstm_layer_size*num_layers \n",
    "\n",
    "# Attention mechanism starts here \n",
    "attn_cntx = concatenate([local_states, RepeatVector(maxlen)(average_active)])\n",
    "attn_cntx = TimeDistributed(Dense(lstm_layer_size, activation='linear', kernel_regularizer=l2(1e-4)))(attn_cntx) \n",
    "attn_cntx = TimeDistributed(BatchNormalization())(attn_cntx) \n",
    "attn_cntx = TimeDistributed(Activation('tanh'))(attn_cntx) \n",
    "attn_cntx = TimeDistributed(Dropout(0.5))(attn_cntx) \n",
    "\n",
    "attn = TimeDistributed(Dense(1, activation='linear', kernel_regularizer=l2(1e-4)))(attn_cntx)\n",
    "attn = Flatten()(attn)\n",
    "attn = Activation('softmax')(attn)\n",
    "attn = Reshape((maxlen, 1))(attn)\n",
    "attn = Lambda(function=lambda x: x, output_shape=lambda shape: (shape[:2] + tuple([state_size])))(attn) # repeats value to make a specific shape\n",
    "\n",
    "final_context = multiply([attn, local_states]) \n",
    "final_context = Lambda(function=lambda x: K.sum(x, axis=1), output_shape=lambda shape: (shape[0],) + shape[2:])(final_context) # eq(2), Ti=1 Î±ihi\n",
    "final_context = Dense(state_size, activation='linear', kernel_regularizer=l2(1e-4))(final_context) \n",
    "final_context = BatchNormalization()(final_context) \n",
    "final_context = Activation('tanh')(final_context) \n",
    "final_context = Dropout(0.5)(final_context) \n",
    "\n",
    "loss_out = Dense(1, activation='sigmoid', name='loss_out')(final_context)\n",
    "model = Model(inputs=[main_input], outputs=[loss_out]) \n",
    "optimizer = Adam(lr=0.001, clipnorm=1.0) \n",
    "model.compile(optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='binary_accuracy', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "model.fit(data, label, batch_size=20, epochs=5, verbose=1, callbacks=[es])\n",
    "result = model.predict(test, batch_size=20, verbose=1)\n",
    "\n",
    "output = []\n",
    "for item in result:\n",
    "    if item >= 0.5:\n",
    "        output.append(1)\n",
    "    else:\n",
    "        output.append(0)      \n",
    "prediction = pd.DataFrame({'category': output})\n",
    "prediction.to_csv('prediction.csv', index=True,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
