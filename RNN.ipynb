{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "from keras.layers import Embedding, ELU, Dropout, Flatten, Input, Dense, BatchNormalization, SpatialDropout1D, LSTM\n",
    "from keras.layers import concatenate, multiply\n",
    "from keras.layers import TimeDistributed, Reshape, RepeatVector, Lambda, Activation\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "train_dict = {}\n",
    "train_index_dict = {}\n",
    "train_label_dict = {}\n",
    "with open('sample_data.csv', 'rb') as f:\n",
    "    index = 0\n",
    "    for sequence in f:\n",
    "        row = sequence.decode(\"utf-8\")\n",
    "        seq_list = row.rstrip().split(',')\n",
    "        size = len(seq_list)\n",
    "        if size not in train_dict:\n",
    "            train_dict[size] = []\n",
    "            train_index_dict[size] = []\n",
    "        train_dict[size].append(seq_list)\n",
    "        train_index_dict[size].append(index)\n",
    "        index += 1\n",
    "        \n",
    "for size in train_dict.keys():\n",
    "    train_dict[size] = np.array(train_dict[size])\n",
    "\n",
    "label = pd.read_csv('sample_label.csv')['category'].values.reshape(-1, 1)\n",
    "for size in train_index_dict.keys():\n",
    "    train_label_dict[size] = label[train_index_dict[4096]]\n",
    "        \n",
    "# data = pd.read_csv('sample_data.csv',header=None,dtype='float',names=list(range(4096)))\n",
    "# test = pd.read_csv('sample_data.csv',header=None,dtype='float',names=list(range(4096)))\n",
    "# label = pd.read_csv('sample_label.csv')['category'].values.reshape(-1, 1)\n",
    "\n",
    "# print(data.head)\n",
    "# print(label.shape)\n",
    "# print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "86/86 [==============================] - 53s 622ms/step - loss: 0.6629 - binary_accuracy: 0.6047\n",
      "Epoch 2/2\n",
      "86/86 [==============================] - 47s 549ms/step - loss: 0.8409 - binary_accuracy: 0.5116\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 15 input samples and 86 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-631eca505d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m# result = model.predict(test, batch_size=20, verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1638\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1499\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    218\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 15 input samples and 86 target samples."
     ]
    }
   ],
   "source": [
    "maxlen=4096\n",
    "embed_size=16\n",
    "lstm_layer_size = 10\n",
    "num_layers = 3\n",
    "\n",
    "main_input = Input(shape=(None,), name='main_input')\n",
    "emb = Embedding(256, embed_size, input_length=maxlen, embeddings_regularizer=l2(1e-4))(main_input)\n",
    "emb = Dropout(0.2)(emb)\n",
    "hs = [] #hidden states from each LSTM layer stored here \n",
    "lstm = LSTM(lstm_layer_size, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(1e-5), recurrent_regularizer=l2(1e-5), return_sequences=True)(emb)\n",
    "hs.append(lstm)\n",
    "for l in range(1, num_layers):\n",
    "    hs.append(LSTM(lstm_layer_size, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(1e-5), recurrent_regularizer=l2(1e-5), return_sequences=True)(hs[-1]))\n",
    "local_states = concatenate(hs)\n",
    "average_active = Lambda(function=lambda x: K.mean(x, axis=1), output_shape=lambda shape: (shape[0],) + shape[2:])(local_states) # this produces h\n",
    "state_size = lstm_layer_size*num_layers \n",
    "\n",
    "# Attention mechanism starts here \n",
    "attn_cntx = concatenate([local_states, RepeatVector(maxlen)(average_active)])\n",
    "attn_cntx = TimeDistributed(Dense(lstm_layer_size, activation='linear', kernel_regularizer=l2(1e-4)))(attn_cntx) \n",
    "attn_cntx = TimeDistributed(BatchNormalization())(attn_cntx) \n",
    "attn_cntx = TimeDistributed(Activation('tanh'))(attn_cntx) \n",
    "attn_cntx = TimeDistributed(Dropout(0.5))(attn_cntx) \n",
    "\n",
    "attn = TimeDistributed(Dense(1, activation='linear', kernel_regularizer=l2(1e-4)))(attn_cntx)\n",
    "attn = Flatten()(attn)\n",
    "attn = Activation('softmax')(attn)\n",
    "attn = Reshape((maxlen, 1))(attn)\n",
    "attn = Lambda(function=lambda x: x, output_shape=lambda shape: (shape[:2] + tuple([state_size])))(attn) # repeats value to make a specific shape\n",
    "\n",
    "final_context = multiply([attn, local_states]) \n",
    "final_context = Lambda(function=lambda x: K.sum(x, axis=1), output_shape=lambda shape: (shape[0],) + shape[2:])(final_context) # eq(2), Ti=1 αihi\n",
    "final_context = Dense(state_size, activation='linear', kernel_regularizer=l2(1e-4))(final_context) \n",
    "final_context = BatchNormalization()(final_context) \n",
    "final_context = Activation('tanh')(final_context) \n",
    "final_context = Dropout(0.5)(final_context) \n",
    "\n",
    "loss_out = Dense(1, activation='sigmoid', name='loss_out')(final_context)\n",
    "model = Model(inputs=[main_input], outputs=[loss_out]) \n",
    "optimizer = Adam(lr=0.001, clipnorm=1.0) \n",
    "model.compile(optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='binary_accuracy', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "for size in train_dict.keys():\n",
    "    model.fit(train_dict[size], train_label_dict[size], batch_size=20, epochs=2, verbose=1, callbacks=[es])\n",
    "# result = model.predict(test, batch_size=20, verbose=1)\n",
    "\n",
    "# output = []\n",
    "# for item in result:\n",
    "#     if item >= 0.5:\n",
    "#         output.append(1)\n",
    "#     else:\n",
    "#         output.append(0)      \n",
    "# prediction = pd.DataFrame({'category': output})\n",
    "# prediction.to_csv('prediction.csv', index=True,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
